metadata: {}

settings:
  logging:
    format: >- # The format of the log message, change HH:mm:ss.SSS to HH:mm:ss for the default loguru format
      <green>{time:YYYY-MM-DD HH:mm:ss}</green> |
      <level>{level: <8}</level> |
      <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> |
      <level>{message}</level>
    log_level: "INFO" # The level of the log messages
    max_length: 800 # The maximum length of the log message in bash
    suffix_length: 200 # The length of the suffix (when truncated)
    log_file:
      enabled: false # default to not log to a file
      path_format: './logs/{basename}_{time:YYYY_MM_DD__HH_mm_ss}'
      # The path to the log file, ext will be added automatically
      log_level: null # default to use the same log level as stdout
    display:
      configs: false # Display the configurations
      configs_update: false # Display the updates of the configurations
      docstring_warning: true # Display the warning message when docstring are excluded
      llm_raw_call_args: false # Display the raw args for the llm calls
      llm_raw_response: false # Display the raw response of the llm calls
      llm_raw_usage: false # Display the raw usage of the llm calls
      llm_call_args: true # Display the args for the llm calls
      llm_usage: true # Display the usage of the llm calls
      llm_response: true # Display the response of the llm calls
      llm_cache: false # Display the cache info
      llm_cost: true # Display the cost of the calls
      tool_calls: true # Display the tool calls
      tool_results: true # Display the results of the tool calls
      display_mode: "live" # The mode to display the streaming output, choices are "live", "print", "none"
      rich:
        language: "markdown" # The language of the rich output
        theme: "monokai" # The theme of the rich output
        line_numbers: false # Whether to display the line numbers
        word_wrap: true # Whether to wrap the words
        refresh_per_second: 4 # The refresh rate of the rich output

  tracing:
    enabled: false # default to not trace the calls
    path_format: './dumps/traces/{basename}_{time:YYYY_MM_DD__HH_mm_ss}'
    # The path to the trace file, ext will be added automatically
    patch_threading: true # whether to patch `threading.Thread`
    strict_match: true # when saving and loading cache, whether need to match the generation id
  concurrency:
    llm_max_workers: 10 # The maximum number of workers for the llm executor
    thread_max_workers: 20 # The maximum number of workers for the threading executor
    process_max_workers: 10 # The maximum number of workers for the processing executor
  messages:
    colors:
      system: red
      user: green
      assistant: cyan
      tool: magenta
  misc:
    suppress_litellm_debug_info: true

prompts:
  continue_generation: >-
    The previous message was cut off due to length limit, please continue to
    complete the message by starting with the last line (marked with
    {last_marker}). Make sure the indentation is correct when continuing.
    Begin your continuation with {last_marker}.
  continue_generation_alt: >-
    The previous message was cut off due to length limit, please continue to
    complete the message by starting with the last part (marked with
    {last_marker}). Make sure the newline and indentation are correct when
    continuing. Begin your continuation with {last_marker}.

# When using APIs through litellm,
#   see the list of available models at https://docs.litellm.ai/docs/providers
# When using SRT server,
#   set model to "srt" and api_base to the address of the server.
servers:
  default: gpt4o-mini
  # create aliases for the models, you may use the model name as server name directly
  gpt4-turbo:
    model: gpt-4-turbo
  gpt4o:
    model: gpt-4o
  gpt4o-mini:
    model: gpt-4o-mini
  _dummy:
    model: _dummy
